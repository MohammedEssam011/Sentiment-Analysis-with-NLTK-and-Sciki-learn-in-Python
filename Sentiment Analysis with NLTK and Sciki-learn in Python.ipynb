{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23441df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraties \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3385638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading data\n",
    "train_tweets =pd.read_csv('./data/train_tweets.csv') \n",
    "test_tweets = pd.read_csv('./data/test_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e1a9518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5030450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet\n",
       "0  31963  #studiolife #aislife #requires #passion #dedic...\n",
       "1  31964   @user #white #supremacists want everyone to s...\n",
       "2  31965  safe ways to heal your #acne!!    #altwaystohe...\n",
       "3  31966  is the hp and the cursed child book up for res...\n",
       "4  31967    3rd #bihday to my amazing, hilarious #nephew..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63379bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_tweets shape :(31962, 3) , label 0:29720 ,label 1 :2242\n"
     ]
    }
   ],
   "source": [
    "print('train_tweets shape :{} , label 0:{} ,label 1 :{}' .format(\n",
    "train_tweets.shape , \n",
    "train_tweets[train_tweets['label']==0].count()['label'],\n",
    "train_tweets[train_tweets['label']==1].count()['label']\n",
    "\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3d0e07",
   "metadata": {},
   "source": [
    "\n",
    "Let’s visualize this data in countplot between the labels with seaborn library. We have only 2242 positive tweets only while 29720 negative tweets. It looks so imbalanced. We have to tackle this problem before processing the data. You wonder why is it a problem?\n",
    "\n",
    "Let’s say we have a machine learning model to classify everything (every tweet) as negative the label 0 in this data, then this classifier seems understanding most of sentiments in tweet texts because most of data are negative in this circumstance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "671b2b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='label', ylabel='count'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x = 'label', data = train_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10c3c7f",
   "metadata": {},
   "source": [
    "To cope with imbalanced data, I choose one of the oversampling methods to generate synthetic samples in the label 1. As a result, the number of label 1 will increase to 29720 that is the same amout of the negative label. But I don’t do that process before preprocessing and convert the tweets to features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f567db1",
   "metadata": {},
   "source": [
    " process documents and sentences, feature engineering?\n",
    "We’ve got raw tweets so we need to do preprocessing and vectorize the instances for algorithm to understand and learn patterns. Here are 5 sub-sections I will follow for data processing and feature engineering.\n",
    "\n",
    "Removal of punctuations\n",
    "Removal of commonly used words (stopwords)\n",
    "Lemmatization\n",
    "Vectorization\n",
    "Oversampling (SMOTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24172250",
   "metadata": {},
   "source": [
    "Removal of punctuations such as %, hashtag #, and @ in tweet is the first step and the second we can remove the frequent used words (stopwords) and at last we will lemmatize the words in the lists. As a result we will be able to have clean lists of lemmatized words without any distractions such as punctuations and stopwords. Let’s pack these 3 features in 1 function with an argument (a raw tweet). We use NLTK library for this purpose. Here’s the sample code. I used TextBlob to remove punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e98dc409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/mohammedessam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/mohammedessam/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mohammedessam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec87c8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def preprocessing (tweet) :\n",
    "    # Generating the list of words in the tweet (hastags and other punctuations removed)\n",
    "    def from_sentence(tweet):\n",
    "        tweet_blob = TextBlob(tweet)\n",
    "        return ' '.join(tweet_blob.words)\n",
    "    \n",
    "    #removing stopwords and words with unusual symbols \n",
    "    def no_user_alpha (tweet):\n",
    "        tweet_list = [ele for ele in tweet.split() if ele != 'user']\n",
    "        clean_tokens = [t for t in tweet_list if re.match(r'[^\\W\\d]*$',t)]\n",
    "        clean_s = ' '.join(clean_tokens)\n",
    "        clean_mess = [word for word in clean_s.split() if word.lower() not in stopwords.words('english') ]\n",
    "        return clean_mess\n",
    "    \n",
    "    # Lemmatize the words in tweets \n",
    "    def lemmatization (tweet_list):\n",
    "        lem = WordNetLemmatizer()\n",
    "        lemmatized_tweet = []\n",
    "        for word in tweet_list :\n",
    "            lemmatized_text = lem.lemmatize(word, 'v')\n",
    "            lemmatized_tweet.append(lemmatized_text)\n",
    "        return lemmatized_tweet\n",
    "    \n",
    "    new_tweet = from_sentence(tweet)\n",
    "    no_punc_tweet = no_user_alpha(new_tweet)\n",
    "    lemmatized_tweet = lemmatization(no_punc_tweet)\n",
    "    \n",
    "    return lemmatized_tweet\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8baeb69",
   "metadata": {},
   "source": [
    "Let’s see how it works with the sample. Print the 10th item of train data in Python interpreter, then next print the preprocessed 10th item, how does it look like? We don’t see punctuations % and #, no stopwords such as in and to in the original tweet. Also unfamilier characters have been removed from the sentence. Sweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9051d1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " â #ireland consumer price index (mom) climbed from previous 0.2% to 0.5% in may   #blog #silver #gold #forex\n"
     ]
    }
   ],
   "source": [
    "print (train_tweets['tweet'].iloc[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0812ae0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ireland', 'consumer', 'price', 'index', 'mom', 'climb', 'previous', 'may', 'blog', 'silver', 'gold', 'forex']\n"
     ]
    }
   ],
   "source": [
    "print (preprocessing(train_tweets['tweet'].iloc[10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aff41a",
   "metadata": {},
   "source": [
    "Let’s move to vectorize the lemmatized tweets. Why vectorize? Because machine learning algorithms can understand only numeric values, so words in string, sentences in text, categorical data must be preprocessed into numbers in advance to train in machine learning algorithms. There are 3 major vectorizing ways for text 1) Frequency Vectors, 2) One-hot encoding, 3) Tfidf Vectors. Let’s use TfidfVectorizer in scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48bd88cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2efbadc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "min_df=2, discard words appearing in less than 2 documents\n",
    "max_df=0.9, discard words appering in more than 90% of the documents\n",
    "sublinear_tf=True, use sublinear weighting\n",
    "use_idf=True, enable IDF\n",
    "\"\"\"\n",
    "\n",
    "vec = TfidfVectorizer(\n",
    "    analyzer = preprocessing,\n",
    "    min_df = 2 ,\n",
    "    max_df = 0.9,\n",
    "    sublinear_tf = True,\n",
    "    use_idf = True\n",
    ")\n",
    "train_vec = vec.fit_transform(train_tweets['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a39015",
   "metadata": {},
   "source": [
    "\n",
    "We’ve got vectorized train data now. It’s time to apply SMOTE to oversample the positive tweets to the same level of the negative tweets. Recall that we have only 2242 positive tweets compared to 29720 negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "163c7e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn import under_sampling, over_sampling\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e06ff56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (31962, 12055), y shape: (31962,)\n",
      "Resampled label balance:\n",
      "0    29720\n",
      "1    29720\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = train_vec\n",
    "y = train_tweets['label']\n",
    "print('X shape: {}, y shape: {}'.format(X.shape, y.shape))\n",
    "\n",
    "sm = SMOTE(sampling_strategy='auto', random_state=39)\n",
    "X_resampled, y_resampled = sm.fit_resample(X, y)\n",
    "print('Resampled label balance:\\n{}'.format(y_resampled.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d679a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (39824, 12055)\n",
      "X_test shape: (19616, 12055)\n",
      "y_train shape: (39824,)\n",
      "y_test shape: (19616,)\n",
      "Result of MultinomialNB\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93      9550\n",
      "           1       0.95      0.92      0.93     10066\n",
      "\n",
      "    accuracy                           0.93     19616\n",
      "   macro avg       0.93      0.93      0.93     19616\n",
      "weighted avg       0.93      0.93      0.93     19616\n",
      "\n",
      "\n",
      "\n",
      "Confusion matrix: \n",
      " [[9044  506]\n",
      " [ 820 9246]]\n",
      "\n",
      "\n",
      "Accuracy score:  0.9324021207177814\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Result of RandomForest\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      9655\n",
      "           1       0.99      0.97      0.98      9961\n",
      "\n",
      "    accuracy                           0.98     19616\n",
      "   macro avg       0.98      0.98      0.98     19616\n",
      "weighted avg       0.98      0.98      0.98     19616\n",
      "\n",
      "\n",
      "\n",
      "Confusion matrix: \n",
      " [[9549  106]\n",
      " [ 315 9646]]\n",
      "\n",
      "\n",
      "Accuracy score:  0.9785379282218597\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Result of GradientBoosting\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.73      0.82     12870\n",
      "           1       0.64      0.92      0.76      6746\n",
      "\n",
      "    accuracy                           0.79     19616\n",
      "   macro avg       0.79      0.83      0.79     19616\n",
      "weighted avg       0.84      0.79      0.80     19616\n",
      "\n",
      "\n",
      "\n",
      "Confusion matrix: \n",
      " [[9350 3520]\n",
      " [ 514 6232]]\n",
      "\n",
      "\n",
      "Accuracy score:  0.7943515497553018\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report ,confusion_matrix,accuracy_score\n",
    "\n",
    "x_train ,x_test, y_train,y_test = train_test_split(X_resampled,y_resampled,test_size=0.33, random_state=39)\n",
    "print('X_train shape: {}'.format(x_train.shape))\n",
    "print('X_test shape: {}'.format(x_test.shape))\n",
    "print('y_train shape: {}'.format(y_train.shape))\n",
    "print('y_test shape: {}'.format(y_test.shape))\n",
    "\n",
    "# pick three algorithms Multinomial Naive Bayes, RandomForest and GradientBoosting\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "clf_mnb =MultinomialNB()\n",
    "clf_rfc =RandomForestClassifier(random_state=39)\n",
    "clf_gbc =GradientBoostingClassifier(random_state=39)\n",
    "clf_names =['MultinomialNB', 'RandomForest', 'GradientBoosting']\n",
    "clf_types=[clf_mnb,clf_rfc,clf_gbc]\n",
    "\n",
    "for i ,clf in enumerate (clf_types):\n",
    "    clf.fit(x_train,y_train.values.ravel())\n",
    "    print('Result of {}\\n'.format(clf_names[i]))\n",
    "    predictions = clf.predict(x_test)\n",
    "    print(classification_report(predictions, y_test))\n",
    "    print('\\n')\n",
    "    print('Confusion matrix: \\n', confusion_matrix(predictions, y_test))\n",
    "    print('\\n')\n",
    "    print('Accuracy score: ', accuracy_score(predictions, y_test))\n",
    "    print('\\n\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1406c490",
   "metadata": {},
   "source": [
    "(Optional) Test the generated model\n",
    "It’s time to play with real world data. I pick 2 Donald Trump’s tweets from his timeline. Let’s use our model to predict tweet’s polarity whether it is positive or negative from vectorized words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38158c5a",
   "metadata": {},
   "source": [
    "We are providing better care, and more choice, at lower cost. We are delivering a healthier, safer, brighter, and more prosperous future for EVERY citizen in our magnificent land – because we are proudly putting AMERICA FIRST! pic.twitter.com/DgeDlpizk1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6f67ad01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['provide', 'better', 'care', 'choice', 'lower', 'cost', 'deliver', 'healthier', 'safer', 'brighter', 'prosperous', 'future', 'EVERY', 'citizen', 'magnificent', 'land', 'proudly', 'put', 'AMERICA', 'FIRST']\n"
     ]
    }
   ],
   "source": [
    "tweet = 'We are providing better care, and more choice, at lower cost. We are delivering a healthier, safer, brighter, and more prosperous future for EVERY citizen in our magnificent land – because we are proudly putting AMERICA FIRST!'\n",
    "preprocess_tweet = preprocessing(tweet)\n",
    "print(preprocess_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "531e0bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative has been predicted for the tweet We are providing better care, and more choice, at lower cost. We are delivering a healthier, safer, brighter, and more prosperous future for EVERY citizen in our magnificent land – because we are proudly putting AMERICA FIRST!\n"
     ]
    }
   ],
   "source": [
    "# vectorize the tweet\n",
    "tweet_vec = vec.transform(pd.Series([tweet]))\n",
    "# predict a label\n",
    "tweet_prediction = clf_rfc.predict(tweet_vec.toarray())\n",
    "tweet_prediction = 'positive' if tweet_prediction[0] == '1' else 'negative'\n",
    "print('{} has been predicted for the tweet {}'.format(tweet_prediction, tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e401b02f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5334d57a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
